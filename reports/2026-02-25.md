# LLM Inference Systems — Paper Scan: 2026-02-25

## Summary

- **Total papers scanned:** 80
- **Papers above relevance threshold (>= 7):** 5

---

### FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill
**Score: 10/10** — Directly addresses LLM inference prefill acceleration with sparse attention on FPGA, targeting TTFT and energy efficiency.
*Authors: Rakshith Jayanth, Viktor Prasanna*
*Published: 2026-02-24*
[arXiv](https://arxiv.org/abs/2602.20515) | [PDF](https://arxiv.org/pdf/2602.20515v1)

> In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's inter...

---

### Scaling State-Space Models on Multiple GPUs with Tensor Parallelism
**Score: 9/10** — Presents tensor parallelism for SSM-based LLM inference with SSM state caching, quantized AllReduce, and multi-GPU throughput scaling.
*Authors: Anurag Dutt, Nimit Shah, Hazem Masarani et al.*
*Published: 2026-02-24*
[arXiv](https://arxiv.org/abs/2602.21144) | [PDF](https://arxiv.org/pdf/2602.21144v1)

> Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and...

---

### ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments
**Score: 9/10** — Directly targets failure recovery in production LLM inference serving (MaaS), avoiding costly model reload and recompilation during inference.
*Authors: Haley Li, Xinglu Wang, Cong Feng et al.*
*Published: 2026-02-24*
[arXiv](https://arxiv.org/abs/2602.21140) | [PDF](https://arxiv.org/pdf/2602.21140v1)

> As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to si...

---

### CHESS: Context-aware Hierarchical Efficient Semantic Selection for Long-Context LLM Inference
**Score: 9/10** — Algorithm-system co-design for KV cache management in long-context LLM inference, achieving 4.56x throughput improvement with 1% KV cache.
*Authors: Chao Fei, Guozhong Li, Chenxi Liu et al.*
*Published: 2026-02-24*
[arXiv](https://arxiv.org/abs/2602.20732) | [PDF](https://arxiv.org/pdf/2602.20732v1)

> Long-context LLMs demand accurate inference at low latency, yet decoding becomes primarily constrained by KV cache as context grows. Prior pruning methods are largely context-agnostic: their token selection ignores step-wise relevance and local se...

---

### TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence
**Score: 9/10** — Hardware accelerator co-designed with ternary quantization for edge LLM inference, addressing the memory wall with hybrid ROM-SRAM architecture.
*Authors: Hongyi Guan, Yijia Zhang, Wenqiang Wang et al.*
*Published: 2026-02-24*
[arXiv](https://arxiv.org/abs/2602.20662) | [PDF](https://arxiv.org/pdf/2602.20662v1)

> The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwi...
