{
  "topic": "LLM inference systems",
  "description": "Serving, optimizing, and scaling large language model inference. Includes KV cache management, speculative decoding, continuous batching, PagedAttention, model quantization for inference, inference frameworks (vLLM, TensorRT-LLM, SGLang, TGI, llama.cpp), inference throughput/latency optimization, distributed inference, and inference hardware.",
  "arxiv_categories": ["cs.LG", "cs.CL", "cs.DC", "cs.PF", "cs.AI", "cs.AR"],
  "keywords": ["inference", "serving", "LLM", "language model", "vLLM", "TensorRT", "quantization", "KV cache", "speculative decoding", "batching", "throughput", "latency", "token generation", "model serving", "llama.cpp", "SGLang"],
  "relevance_threshold": 7,
  "max_papers": 100
}
