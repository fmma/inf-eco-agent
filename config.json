{
  "topic": "LLM inference systems and I/O",
  "description": "Serving, optimizing, and scaling large language model inference. Includes KV cache management, speculative decoding, continuous batching, PagedAttention, model quantization for inference, inference frameworks (vLLM, TensorRT-LLM, SGLang, TGI, llama.cpp), inference throughput/latency optimization, distributed inference, inference hardware, and I/O optimization (disk I/O, network I/O, memory bandwidth, data loading, storage systems, NVMe, RDMA, interconnects) as it relates to LLM inference and serving.",
  "arxiv_categories": ["cs.LG", "cs.CL", "cs.DC", "cs.PF", "cs.AI", "cs.AR"],
  "keywords": ["inference", "serving", "LLM", "language model", "vLLM", "TensorRT", "quantization", "KV cache", "speculative decoding", "batching", "throughput", "latency", "token generation", "model serving", "llama.cpp", "SGLang", "I/O", "memory bandwidth", "NVMe", "RDMA", "interconnect", "data transfer", "disk I/O"],
  "relevance_threshold": 7,
  "max_papers": 100
}
